{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Py2K \u00b6 A high level Python to Kafka API with Schema Registry compatibility and automatic avro schema creation. Free software: Apache2 license Installation \u00b6 Py2K is currently available on PIP: pip install py2k Contributing \u00b6 Please see the Contribution Guide for more information. Usage \u00b6 Minimal Example \u00b6 from py2k.record import PandasToRecordsTransformer from py2k.writer import KafkaWriter # assuming we have a pandas DataFrame, df record_transformer = PandasToRecordsTransformer ( df = df , record_name = 'test_model' ) records = record_transformer . from_pandas () writer = KafkaWriter ( topic = \"topic_name\" , schema_registry_config = schema_registry_config , producer_config = producer_config ) writer . write ( records ) Features \u00b6 Schema Registry Integration Automatic Avro Serialization from pandas DataFrames Automatic Avro Schema generation from pandas DataFrames and Pydantic objects License \u00b6 Copyright 2021 ABSA Group Limited Licensed under the Apache License , Version 2 . 0 ( the \" License \" ) ; you may not use this file except in compliance with the License . You may obtain a copy of the License at http : // www . apache . org / licenses / LICENSE - 2 . 0 Unless required by applicable law or agreed to in writing , software distributed under the License is distributed on an \" AS IS \" BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . See the License for the specific language governing permissions and limitations under the License .","title":"Py2K"},{"location":"#welcome-to-py2k","text":"A high level Python to Kafka API with Schema Registry compatibility and automatic avro schema creation. Free software: Apache2 license","title":"Welcome to Py2K"},{"location":"#installation","text":"Py2K is currently available on PIP: pip install py2k","title":"Installation"},{"location":"#contributing","text":"Please see the Contribution Guide for more information.","title":"Contributing"},{"location":"#usage","text":"","title":"Usage"},{"location":"#minimal-example","text":"from py2k.record import PandasToRecordsTransformer from py2k.writer import KafkaWriter # assuming we have a pandas DataFrame, df record_transformer = PandasToRecordsTransformer ( df = df , record_name = 'test_model' ) records = record_transformer . from_pandas () writer = KafkaWriter ( topic = \"topic_name\" , schema_registry_config = schema_registry_config , producer_config = producer_config ) writer . write ( records )","title":"Minimal Example"},{"location":"#features","text":"Schema Registry Integration Automatic Avro Serialization from pandas DataFrames Automatic Avro Schema generation from pandas DataFrames and Pydantic objects","title":"Features"},{"location":"#license","text":"Copyright 2021 ABSA Group Limited Licensed under the Apache License , Version 2 . 0 ( the \" License \" ) ; you may not use this file except in compliance with the License . You may obtain a copy of the License at http : // www . apache . org / licenses / LICENSE - 2 . 0 Unless required by applicable law or agreed to in writing , software distributed under the License is distributed on an \" AS IS \" BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either express or implied . See the License for the specific language governing permissions and limitations under the License .","title":"License"},{"location":"motivation/","text":"Why we built Py2K \u00b6 Py2K is a library that connects pandas dataframes to Kafka and Schema Registry. It is a tool designed by data scientists, for data scientists . It allows data scientists, data engineers and developers to stay within the Python environment and avoid the jargon related to Kafka, Schema Registry, and Avro by utilising Pythonic Objects. If you look online for examples of how to post data from pandas onto Kafka with integration into the Confluent Schema Registry, you won't find many of them. While well documented for general Kafka Producers, they become increasingly complex to implement, which can easily lead you to write more code for getting your data to Kafka than the actual project use-case. Remove Bloat \u00b6 We wanted to create a tool with minimal overhead that didn't bloat data science projects with copious amounts of boilerplate code, effectively taking the process from 100s of lines of code to just 4 lines! (8 if we want to be PEP8 compliant) Keep Data Scientists doing Data Science \u00b6 We wanted to keep data scientists using tools they're comfortable with , so we provide an API for automatic conversion of pandas Dataframes . This is analogous to Absa's ABRiS package for Spark Dataframes which helps data engineers by providing similar functionality. Minimal Kafka, Avro, Confluent Jargon \u00b6 We wanted to abstract data scientists from the tedious job of serializing their data into Avro objects and defining Avro schema for each of their products. Utilising pydantic we're able to automate schema validation and generation for data. Usable and Maintained Documentation and Examples \u00b6 Lastly, we wanted Py2K to have usable documentation and examples. As we build out our documentation, each bit of python code has unit tests to ensure that the code provided works for the latest version of Py2K .","title":"Motivation"},{"location":"motivation/#why-we-built-py2k","text":"Py2K is a library that connects pandas dataframes to Kafka and Schema Registry. It is a tool designed by data scientists, for data scientists . It allows data scientists, data engineers and developers to stay within the Python environment and avoid the jargon related to Kafka, Schema Registry, and Avro by utilising Pythonic Objects. If you look online for examples of how to post data from pandas onto Kafka with integration into the Confluent Schema Registry, you won't find many of them. While well documented for general Kafka Producers, they become increasingly complex to implement, which can easily lead you to write more code for getting your data to Kafka than the actual project use-case.","title":"Why we built Py2K"},{"location":"motivation/#remove-bloat","text":"We wanted to create a tool with minimal overhead that didn't bloat data science projects with copious amounts of boilerplate code, effectively taking the process from 100s of lines of code to just 4 lines! (8 if we want to be PEP8 compliant)","title":"Remove Bloat"},{"location":"motivation/#keep-data-scientists-doing-data-science","text":"We wanted to keep data scientists using tools they're comfortable with , so we provide an API for automatic conversion of pandas Dataframes . This is analogous to Absa's ABRiS package for Spark Dataframes which helps data engineers by providing similar functionality.","title":"Keep Data Scientists doing Data Science"},{"location":"motivation/#minimal-kafka-avro-confluent-jargon","text":"We wanted to abstract data scientists from the tedious job of serializing their data into Avro objects and defining Avro schema for each of their products. Utilising pydantic we're able to automate schema validation and generation for data.","title":"Minimal Kafka, Avro, Confluent Jargon"},{"location":"motivation/#usable-and-maintained-documentation-and-examples","text":"Lastly, we wanted Py2K to have usable documentation and examples. As we build out our documentation, each bit of python code has unit tests to ensure that the code provided works for the latest version of Py2K .","title":"Usable and Maintained Documentation and Examples"},{"location":"reference/","text":"Reference \u00b6 py2k.writer.KafkaWriter \u00b6 __init__ ( self , topic , schema_registry_config , producer_config ) special \u00b6 A class for easy writing of data to kafka Parameters: Name Type Description Default topic str topic to post to required schema_registry_config Dict[str, Any] a dictionary compatible with the confluent_kafka.schema_registry.SchemaRegistryClient required producer_config Dict[str, Any] a dictionary compatible with the confluent_kafka.SerializingProducer required write ( self , records , verbose = False ) \u00b6 Writes data to Kafka. Parameters: Name Type Description Default records Iterable[KafkaRecord] Serialized KafkaModel objects required verbose bool Whether or not you want to show the loading bar False Examples: >>> from py2k.writer import KafkaWriter >>> writer = KafkaWriter ( topic = topic , schema_registry_config = schema_registry_config , producer_config = producer_config ) >>> writer . write ( records ) 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 00 < 00 : 00 , 7.69 it / s ] py2k.record.PandasToRecordsTransformer \u00b6 class model for automatic serialization of Pandas DataFrame to KafkaRecord __init__ ( self , df , record_name , fields_defaults = None , types_defaults = None , optional_fields = None , key_fields = None , include_key = None ) special \u00b6 Parameters: Name Type Description Default df pd.DataFrame Pandas dataframe to serialize required record_name str destination Pydantic model required fields_defaults Dict[str, object] default values for fields in the dataframe. The keys are the fields names. Defaults to None. None types_defaults Dict[object, object] default values for the types in the dataframe. The keys are the types, e.g. int. Defaults to None. None optional_fields List[str] list of fields which should be marked as optional. Defaults to None. None key_fields Set[str] set of fields which are meant to be key of the schema None include_key bool bool: Indicator whether key fields should be included in value None Source code in py2k/record.py def __init__ ( self , df : pd . DataFrame , record_name : str , fields_defaults : Dict [ str , object ] = None , types_defaults : Dict [ object , object ] = None , optional_fields : List [ str ] = None , key_fields : Set [ str ] = None , include_key : bool = None ): \"\"\" Args: df (pd.DataFrame): Pandas dataframe to serialize record_name (str): destination Pydantic model fields_defaults (Dict[str, object], optional): default values for fields in the dataframe. The keys are the fields names. Defaults to None. types_defaults (Dict[object, object], optional): default values for the types in the dataframe. The keys are the types, e.g. int. Defaults to None. optional_fields (List[str], optional): list of fields which should be marked as optional. Defaults to None. key_fields (Set[str], optional): set of fields which are meant to be key of the schema include_key: bool: Indicator whether key fields should be included in value \"\"\" self . _df = df _class = self . _class ( key_fields , include_key ) model_creator = PandasModelCreator ( df , record_name , fields_defaults , types_defaults , optional_fields , _class ) self . _model = model_creator . create () from_pandas ( self , df = None ) \u00b6 Creates list of KafkaModel objects from a pandas DataFrame Parameters: Name Type Description Default df pd.DataFrame Pandas dataframe. Defaults to None. None Returns: Type Description List[KafkaModel] serialized list of KafkaModel objects Examples: >>> record_transformer = PandasToRecordsTransformer ( df = df , record_name = 'KafkaRecord' ) >>> record_transformer . from_pandas () [ KafkaRecord ( name = 'Daniel' , cool_level = 'low' , value = 27.1 , date = datetime . date ( 2021 , 3 , 1 ))] Source code in py2k/record.py def from_pandas ( self , df : pd . DataFrame = None ) -> List [ 'KafkaRecord' ]: \"\"\"Creates list of KafkaModel objects from a pandas DataFrame Args: df (pd.DataFrame): Pandas dataframe. Defaults to None. Returns: List[KafkaModel]: serialized list of KafkaModel objects Examples: >>> record_transformer = PandasToRecordsTransformer(df=df, record_name='KafkaRecord') >>> record_transformer.from_pandas() [KafkaRecord(name='Daniel', cool_level='low', value=27.1, date=datetime.date(2021, 3, 1))] \"\"\" if df is not None : return self . _model . from_pandas ( df ) return self . _model . from_pandas ( self . _df ) iter_from_pandas ( self , df = None ) \u00b6 Creates iterator of KafkaModel objects from a pandas DataFrame Parameters: Name Type Description Default df pd.DataFrame Pandas dataframe. Defaults to None. None Returns: Type Description Iterator[KafkaModel] serialized list of KafkaModel objects Examples: >>> record_transformer = PandasToRecordsTransformer ( df = df , record_name = 'KafkaRecord' ) >>> record_transformer . iter_from_pandas () Source code in py2k/record.py def iter_from_pandas ( self , df : pd . DataFrame = None ): \"\"\"Creates iterator of KafkaModel objects from a pandas DataFrame Args: df (pd.DataFrame): Pandas dataframe. Defaults to None. Returns: Iterator[KafkaModel]: serialized list of KafkaModel objects Examples: >>> record_transformer = PandasToRecordsTransformer(df=df, record_name='KafkaRecord') >>> record_transformer.iter_from_pandas() \"\"\" if df is not None : return self . _model . iter_from_pandas ( df ) return self . _model . iter_from_pandas ( self . _df )","title":"API Reference"},{"location":"reference/#reference","text":"","title":"Reference"},{"location":"reference/#py2k.writer.KafkaWriter","text":"","title":"KafkaWriter"},{"location":"reference/#py2k.writer.KafkaWriter.__init__","text":"A class for easy writing of data to kafka Parameters: Name Type Description Default topic str topic to post to required schema_registry_config Dict[str, Any] a dictionary compatible with the confluent_kafka.schema_registry.SchemaRegistryClient required producer_config Dict[str, Any] a dictionary compatible with the confluent_kafka.SerializingProducer required","title":"__init__()"},{"location":"reference/#py2k.writer.KafkaWriter.write","text":"Writes data to Kafka. Parameters: Name Type Description Default records Iterable[KafkaRecord] Serialized KafkaModel objects required verbose bool Whether or not you want to show the loading bar False Examples: >>> from py2k.writer import KafkaWriter >>> writer = KafkaWriter ( topic = topic , schema_registry_config = schema_registry_config , producer_config = producer_config ) >>> writer . write ( records ) 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 00 < 00 : 00 , 7.69 it / s ]","title":"write()"},{"location":"reference/#py2k.record.PandasToRecordsTransformer","text":"class model for automatic serialization of Pandas DataFrame to KafkaRecord","title":"PandasToRecordsTransformer"},{"location":"reference/#py2k.record.PandasToRecordsTransformer.__init__","text":"Parameters: Name Type Description Default df pd.DataFrame Pandas dataframe to serialize required record_name str destination Pydantic model required fields_defaults Dict[str, object] default values for fields in the dataframe. The keys are the fields names. Defaults to None. None types_defaults Dict[object, object] default values for the types in the dataframe. The keys are the types, e.g. int. Defaults to None. None optional_fields List[str] list of fields which should be marked as optional. Defaults to None. None key_fields Set[str] set of fields which are meant to be key of the schema None include_key bool bool: Indicator whether key fields should be included in value None Source code in py2k/record.py def __init__ ( self , df : pd . DataFrame , record_name : str , fields_defaults : Dict [ str , object ] = None , types_defaults : Dict [ object , object ] = None , optional_fields : List [ str ] = None , key_fields : Set [ str ] = None , include_key : bool = None ): \"\"\" Args: df (pd.DataFrame): Pandas dataframe to serialize record_name (str): destination Pydantic model fields_defaults (Dict[str, object], optional): default values for fields in the dataframe. The keys are the fields names. Defaults to None. types_defaults (Dict[object, object], optional): default values for the types in the dataframe. The keys are the types, e.g. int. Defaults to None. optional_fields (List[str], optional): list of fields which should be marked as optional. Defaults to None. key_fields (Set[str], optional): set of fields which are meant to be key of the schema include_key: bool: Indicator whether key fields should be included in value \"\"\" self . _df = df _class = self . _class ( key_fields , include_key ) model_creator = PandasModelCreator ( df , record_name , fields_defaults , types_defaults , optional_fields , _class ) self . _model = model_creator . create ()","title":"__init__()"},{"location":"reference/#py2k.record.PandasToRecordsTransformer.from_pandas","text":"Creates list of KafkaModel objects from a pandas DataFrame Parameters: Name Type Description Default df pd.DataFrame Pandas dataframe. Defaults to None. None Returns: Type Description List[KafkaModel] serialized list of KafkaModel objects Examples: >>> record_transformer = PandasToRecordsTransformer ( df = df , record_name = 'KafkaRecord' ) >>> record_transformer . from_pandas () [ KafkaRecord ( name = 'Daniel' , cool_level = 'low' , value = 27.1 , date = datetime . date ( 2021 , 3 , 1 ))] Source code in py2k/record.py def from_pandas ( self , df : pd . DataFrame = None ) -> List [ 'KafkaRecord' ]: \"\"\"Creates list of KafkaModel objects from a pandas DataFrame Args: df (pd.DataFrame): Pandas dataframe. Defaults to None. Returns: List[KafkaModel]: serialized list of KafkaModel objects Examples: >>> record_transformer = PandasToRecordsTransformer(df=df, record_name='KafkaRecord') >>> record_transformer.from_pandas() [KafkaRecord(name='Daniel', cool_level='low', value=27.1, date=datetime.date(2021, 3, 1))] \"\"\" if df is not None : return self . _model . from_pandas ( df ) return self . _model . from_pandas ( self . _df )","title":"from_pandas()"},{"location":"reference/#py2k.record.PandasToRecordsTransformer.iter_from_pandas","text":"Creates iterator of KafkaModel objects from a pandas DataFrame Parameters: Name Type Description Default df pd.DataFrame Pandas dataframe. Defaults to None. None Returns: Type Description Iterator[KafkaModel] serialized list of KafkaModel objects Examples: >>> record_transformer = PandasToRecordsTransformer ( df = df , record_name = 'KafkaRecord' ) >>> record_transformer . iter_from_pandas () Source code in py2k/record.py def iter_from_pandas ( self , df : pd . DataFrame = None ): \"\"\"Creates iterator of KafkaModel objects from a pandas DataFrame Args: df (pd.DataFrame): Pandas dataframe. Defaults to None. Returns: Iterator[KafkaModel]: serialized list of KafkaModel objects Examples: >>> record_transformer = PandasToRecordsTransformer(df=df, record_name='KafkaRecord') >>> record_transformer.iter_from_pandas() \"\"\" if df is not None : return self . _model . iter_from_pandas ( df ) return self . _model . iter_from_pandas ( self . _df )","title":"iter_from_pandas()"},{"location":"release_notes/","text":"Release Notes \u00b6 Latest Changes \u00b6 v1.9.1 (2021-10-04) \u00b6 Bug \u00b6 \ud83d\udc1b Kafka Schema was not extracted from entire Dataframe. #76 - @vesely-david v1.9.0 (2021-05-15) \u00b6 Enhancement \u00b6 \u2728 Support for dynamic creation to return an iterator. #67 - @vesely-david User can choose whether to return list (from_pandas) or iterator (iter_from_pandas) KafkaWriter was adjusted accordingly to accept any iterable in it's write method v1.8.2 (2021-04-06) \u00b6 Bugs \u00b6 Resolved boolean schema not being converted to the correct avro schema values PR #48 - @vesely-david v1.8.1 (2021-03-31) \u00b6 Docs \u00b6 Added examples and solved mkdocs gitub.io page build - @DanWertheimer . PR #45 v1.8.0 (2021-03-29) \u00b6 Fixes \u00b6 Adhering to Kafka and Avro parlance by renaming: models module -> record KafkaModel -> KafkaRecord DynamicPandasModel -> PandasToRecordsTransformer item -> record Move schema knowledge to KafkaRecord Introduce __key_fields__ in KafkaRecord to enable specifying which fields are part of the key Introduce __include_key__ in KafkaRecord to enable specifying whether key_fields should be part of the value message Big thank you to @vesely-david for this change v1.7.0 (2021-03-11) \u00b6 Minor API change for easier dynamic creation of KafkaModels from a pandas DataFrame v1.6.0 (2021-03-01) \u00b6 First commit on Github.","title":"Release Notes"},{"location":"release_notes/#release-notes","text":"","title":"Release Notes"},{"location":"release_notes/#latest-changes","text":"","title":"Latest Changes"},{"location":"release_notes/#v191-2021-10-04","text":"","title":"v1.9.1 (2021-10-04)"},{"location":"release_notes/#bug","text":"\ud83d\udc1b Kafka Schema was not extracted from entire Dataframe. #76 - @vesely-david","title":"Bug"},{"location":"release_notes/#v190-2021-05-15","text":"","title":"v1.9.0 (2021-05-15)"},{"location":"release_notes/#enhancement","text":"\u2728 Support for dynamic creation to return an iterator. #67 - @vesely-david User can choose whether to return list (from_pandas) or iterator (iter_from_pandas) KafkaWriter was adjusted accordingly to accept any iterable in it's write method","title":"Enhancement"},{"location":"release_notes/#v182-2021-04-06","text":"","title":"v1.8.2 (2021-04-06)"},{"location":"release_notes/#bugs","text":"Resolved boolean schema not being converted to the correct avro schema values PR #48 - @vesely-david","title":"Bugs"},{"location":"release_notes/#v181-2021-03-31","text":"","title":"v1.8.1 (2021-03-31)"},{"location":"release_notes/#docs","text":"Added examples and solved mkdocs gitub.io page build - @DanWertheimer . PR #45","title":"Docs"},{"location":"release_notes/#v180-2021-03-29","text":"","title":"v1.8.0 (2021-03-29)"},{"location":"release_notes/#fixes","text":"Adhering to Kafka and Avro parlance by renaming: models module -> record KafkaModel -> KafkaRecord DynamicPandasModel -> PandasToRecordsTransformer item -> record Move schema knowledge to KafkaRecord Introduce __key_fields__ in KafkaRecord to enable specifying which fields are part of the key Introduce __include_key__ in KafkaRecord to enable specifying whether key_fields should be part of the value message Big thank you to @vesely-david for this change","title":"Fixes"},{"location":"release_notes/#v170-2021-03-11","text":"Minor API change for easier dynamic creation of KafkaModels from a pandas DataFrame","title":"v1.7.0 (2021-03-11)"},{"location":"release_notes/#v160-2021-03-01","text":"First commit on Github.","title":"v1.6.0 (2021-03-01)"},{"location":"tutorial/","text":"Tutorial - User Guide - Intro \u00b6 This tutorial will show you how to use Py2K and it's features in a step-by-step manner. All the Python code in the tutorial should run as-is in your environment, provided you have the correct configurations for your Kafka cluster. Run the code \u00b6 All the code blocks can be copied and used directly (they are actually tested Python files). These can be run by copying the files into any python file, call it py2k_test.py and run it with python py2k_test.py . This will show you how easy it is to use Py2K with its minimal code style. Install Py2K \u00b6 The first step is to install Py2K: $ pip install py2k ---> 100%","title":"Tutorial - User Guide - Intro"},{"location":"tutorial/#tutorial-user-guide-intro","text":"This tutorial will show you how to use Py2K and it's features in a step-by-step manner. All the Python code in the tutorial should run as-is in your environment, provided you have the correct configurations for your Kafka cluster.","title":"Tutorial - User Guide - Intro"},{"location":"tutorial/#run-the-code","text":"All the code blocks can be copied and used directly (they are actually tested Python files). These can be run by copying the files into any python file, call it py2k_test.py and run it with python py2k_test.py . This will show you how easy it is to use Py2K with its minimal code style.","title":"Run the code"},{"location":"tutorial/#install-py2k","text":"The first step is to install Py2K: $ pip install py2k ---> 100%","title":"Install Py2K"},{"location":"tutorial/first_steps/","text":"First Steps \u00b6 KafkaRecord \u00b6 The simplest KafkaRecord can be defined through inheritance of the KafkaRecord class: import datetime from py2k.record import KafkaRecord class MyRecord ( KafkaRecord ): name : str age : int birthday : datetime . date The KafkaRecord makes use of Pydantic models to allow for automatic avro schema generation. KafkaWriter \u00b6 The KafkaWriter is the object responsible for take a list of KafkaRecords and posting them onto Kafka. Using the KafkaWriter only requires that you define your schema registry config and producer config that is compatible with the confluent_kafka API . Specifically: SchemaRegistryClient SerializingProducer Note Additionally, you can have a look at the librdkafka documentation for addtional configuration options or the Confluent Schema Registry documentation The KafkaWriter can be used as follows: from py2k.writer import KafkaWriter writer = KafkaWriter ( topic = 'dummy_topic' , schema_registry_config = { 'url' : 'http://myschemaregistry.com:8081' }, producer_config = { 'bootstrap.servers' : 'myproducer.com:9092' } ) In practice, copy this and put it into a file called test_py2k.py and fill in your kafka configuration. import datetime from py2k.record import KafkaRecord class MyRecord ( KafkaRecord ): name : str age : int birthday : datetime . date record = MyRecord ( ** { 'name' : 'Dan' , 'age' : 27 , 'birthday' : datetime . date ( 1993 , 9 , 4 )}) from py2k.writer import KafkaWriter writer = KafkaWriter ( topic = 'dummy_topic' , schema_registry_config = { 'url' : 'http://myschemaregistry.com:8081' }, producer_config = { 'bootstrap.servers' : 'myproducer.com:9092' } ) writer . write ([ record ]) running this: $ python test_py2k.py 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 7.69it/s] // It has now pushed that record onto Kafka","title":"First Steps"},{"location":"tutorial/first_steps/#first-steps","text":"","title":"First Steps"},{"location":"tutorial/first_steps/#kafkarecord","text":"The simplest KafkaRecord can be defined through inheritance of the KafkaRecord class: import datetime from py2k.record import KafkaRecord class MyRecord ( KafkaRecord ): name : str age : int birthday : datetime . date The KafkaRecord makes use of Pydantic models to allow for automatic avro schema generation.","title":"KafkaRecord"},{"location":"tutorial/first_steps/#kafkawriter","text":"The KafkaWriter is the object responsible for take a list of KafkaRecords and posting them onto Kafka. Using the KafkaWriter only requires that you define your schema registry config and producer config that is compatible with the confluent_kafka API . Specifically: SchemaRegistryClient SerializingProducer Note Additionally, you can have a look at the librdkafka documentation for addtional configuration options or the Confluent Schema Registry documentation The KafkaWriter can be used as follows: from py2k.writer import KafkaWriter writer = KafkaWriter ( topic = 'dummy_topic' , schema_registry_config = { 'url' : 'http://myschemaregistry.com:8081' }, producer_config = { 'bootstrap.servers' : 'myproducer.com:9092' } ) In practice, copy this and put it into a file called test_py2k.py and fill in your kafka configuration. import datetime from py2k.record import KafkaRecord class MyRecord ( KafkaRecord ): name : str age : int birthday : datetime . date record = MyRecord ( ** { 'name' : 'Dan' , 'age' : 27 , 'birthday' : datetime . date ( 1993 , 9 , 4 )}) from py2k.writer import KafkaWriter writer = KafkaWriter ( topic = 'dummy_topic' , schema_registry_config = { 'url' : 'http://myschemaregistry.com:8081' }, producer_config = { 'bootstrap.servers' : 'myproducer.com:9092' } ) writer . write ([ record ]) running this: $ python test_py2k.py 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 7.69it/s] // It has now pushed that record onto Kafka","title":"KafkaWriter"},{"location":"tutorial/security/","text":"Security \u00b6 Py2K is built on top of the confluent-kafka python package which uses librdkafka configuration for authentication. SSL Authentication Example \u00b6 SSL authentication is built into the base Py2K wheels. As an example, the below config and setup will work: from py2k.writer import KafkaWriter cert_config = { 'ssl.ca.location' : '/path/to/ca.pem' , 'ssl.certificate.location' : '/path/to/cert.pem' , 'ssl.key.location' : '/path/to/ssl.key' , } topic = 'mytopic' schema_registry_config = { 'url' : 'https://schemaregistry.com' , ** cert_config } producer_config = { 'bootstrap.servers' : 'bootstrapservers.com' , 'security.protocol' : 'ssl' , ** cert_config , } writer = KafkaWriter ( topic = topic , schema_registry_config = schema_registry_config , producer_config = producer_config , ) SASL_SSL Kerberos Authentication Example \u00b6 Info The Py2K installation install confluent-kafka for you, however the base confluent-kafka librdkafka linux wheel is not built with SASL Kerberos/GSSAPI support and if you required this you will need to install the wheels on your system first. For a guide, see here Once you've built from source you can use a similar base config to below: from py2k.writer import KafkaWriter cert_config = { 'ssl.ca.location' : '/path/to/ca.pem' , 'ssl.certificate.location' : '/path/to/cert.pem' , 'ssl.key.location' : '/path/to/ssl.key' , } topic = 'mytopic' schema_registry_config = { 'url' : 'https://schemaregistry.com' , ** cert_config } producer_config = { 'bootstrap.servers' : 'bootstrapservers.com' , 'security.protocol' : 'SASL_SSL' , 'sasl.kerberos.principal' : 'principal@DOMAIN' , 'sasl.kerberos.keytab' : '/path/to/principal.keytab' , ** cert_config , } writer = KafkaWriter ( topic = topic , schema_registry_config = schema_registry_config , producer_config = producer_config , )","title":"Security"},{"location":"tutorial/security/#security","text":"Py2K is built on top of the confluent-kafka python package which uses librdkafka configuration for authentication.","title":"Security"},{"location":"tutorial/security/#ssl-authentication-example","text":"SSL authentication is built into the base Py2K wheels. As an example, the below config and setup will work: from py2k.writer import KafkaWriter cert_config = { 'ssl.ca.location' : '/path/to/ca.pem' , 'ssl.certificate.location' : '/path/to/cert.pem' , 'ssl.key.location' : '/path/to/ssl.key' , } topic = 'mytopic' schema_registry_config = { 'url' : 'https://schemaregistry.com' , ** cert_config } producer_config = { 'bootstrap.servers' : 'bootstrapservers.com' , 'security.protocol' : 'ssl' , ** cert_config , } writer = KafkaWriter ( topic = topic , schema_registry_config = schema_registry_config , producer_config = producer_config , )","title":"SSL Authentication Example"},{"location":"tutorial/security/#sasl_ssl-kerberos-authentication-example","text":"Info The Py2K installation install confluent-kafka for you, however the base confluent-kafka librdkafka linux wheel is not built with SASL Kerberos/GSSAPI support and if you required this you will need to install the wheels on your system first. For a guide, see here Once you've built from source you can use a similar base config to below: from py2k.writer import KafkaWriter cert_config = { 'ssl.ca.location' : '/path/to/ca.pem' , 'ssl.certificate.location' : '/path/to/cert.pem' , 'ssl.key.location' : '/path/to/ssl.key' , } topic = 'mytopic' schema_registry_config = { 'url' : 'https://schemaregistry.com' , ** cert_config } producer_config = { 'bootstrap.servers' : 'bootstrapservers.com' , 'security.protocol' : 'SASL_SSL' , 'sasl.kerberos.principal' : 'principal@DOMAIN' , 'sasl.kerberos.keytab' : '/path/to/principal.keytab' , ** cert_config , } writer = KafkaWriter ( topic = topic , schema_registry_config = schema_registry_config , producer_config = producer_config , )","title":"SASL_SSL Kerberos Authentication Example"}]}